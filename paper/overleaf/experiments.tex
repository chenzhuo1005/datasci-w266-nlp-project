\section{Experiments}

The main focus of the experiment was to evaluate the performance of LLMs by comparing the accuracy of answers generated with and without knowledge retrieval augmentation. Each LLM mentioned in the Model Selection section was tested in pairs. The first module simply generated text based on each question in the \emph{nq\_open} dataset. Given the different pre-training and fine-tuning of the LLMs, a specific prompt was chosen for each to conduct question-answering inference tasks (see~\cref{sec:prompt_engineering}). The second module added a knowledge retrieval step by setting up a RAG module using \emph{LlamaIndex} APIs. The input question was first embedded by the embedding model and then a vector search was conducted for similar embedded questions in the \emph{Pinecone} vector database, which stores \emph{PAQ} knowledge. The top-k most similar question-answer pairs, along with their similarity scores, were retrieved. The hyper-parameter \textbf{K} was set to 10 for this experiment. Finally, the embedded questions and retrieved knowledge question-answer pairs were fed into the LLMs to generate answers.

\input{tables/flan_t5_base_qa}

\paragraph{Evaluation Metrics} \emph{Exact Match (EM)} scores are the primary metric reported in this experiment, as they indicate a complete match with the generated answer. For questions in the validation dataset that have multiple answers (which is quite common, as there are often different ways to answer the same question), a generated answer is considered a successful exact match if it completely matches any of the answers in the list of multiple answers. In addition to \emph{EM}, \emph{F1 scores} and \emph{Rouge scores} are also reported as supplementary metrics.

\paragraph{Baselines} \emph{Flan-T5-base} was selected as the baseline language model for this experiment due to its minimal number of parameters and simplest structure. The original \emph{T5-base} model was not chosen because it failed to generate reasonable answers, with or without \emph{RAG}, despite various prompt engineering trials. \emph{Flan-T5-base}, fine-tuned with instructions, provided reasonable baseline metrics after experimentation and was therefore chosen. \cref{tab:flan_t5_base_qa} shows top 5 questions from the validation dataset answered by the \emph{Flan-T5-base} model, with and without \emph{PAQ} knowledge.