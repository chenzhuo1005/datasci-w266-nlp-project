\section{Results}

\input{tables/rag_results}

\cref{tab:rag_results} displays the experimental results on the \emph{nq\_open} validation datasets. The \emph{EM}, \emph{F1}, \emph{Rouge1}, and \emph{RougeL} scores are reported for each large language model along with its paired RAG-enhanced version. The baseline model, \emph{Flan-T5-base} with RAG enhancement, reported an \emph{EM} of $27.59$, which represents a substantial increase of $23.07$ percentage points compared to its plain version without knowledge augmentation. The \emph{F1} increased by $24.43$ percentage points $(7.84 \rightarrow 32.27)$, and the increases for \emph{Rouge1} and \emph{RougeL} were $18.87~(5.78 \rightarrow 24.65)$ and $18.83 ~(5.78 \rightarrow 24.61)$, respectively.

The next model, \emph{Flan-T5-large}, initially reported higher metrics compared to \emph{Flan-T5-base} without RAG, with \emph{EM}, \emph{F1}, \emph{Rouge1}, and \emph{RougeL} at $8.45$, $12.63$, $9.72$, and $9.72$, respectively. This improvement can be attributed to the larger model size (770M vs 220M parameters), which allows for better understanding of the prompted questions and stores more parameterized knowledge. With RAG enhancement, \emph{Flan-T5-large} also demonstrated substantial improvement, with increases of \emph{EM} $27.03~(8.45 \rightarrow 35.48)$, \emph{F1} $27.32~(12.63 \rightarrow 39.95)$, \emph{Rouge1} $20.76~(9.72 \rightarrow 30.48)$, and \emph{RougeL} $20.78~(9.72 \rightarrow 30.5)$ percentage points, respectively.

\emph{Llama-2-13B-chat-GGUF}, a 13-billion parameter variant of the Llama 2 model, exhibited strong performance in answering \emph{nq\_open} questions without external knowledge, presenting \emph{EM}, \emph{F1}, \emph{Rouge1}, and \emph{RougeL} scores of $35.96$, $40.16$, $30.4$, and $30.22$, respectively. With RAG enhancement, \emph{Llama-2-13B-chat-GGUF} showed slight improvements, with increases in \emph{EM}, \emph{F1}, \emph{Rouge1}, and \emph{RougeL} of $1.66~(35.96 \rightarrow 37.62)$, $0.57~(40.16 \rightarrow 40.73)$, $0.57~(30.4 \rightarrow 30.97)$, and $0.73~(30.22 \rightarrow 30.95)$ pencentage points, respectively. These results indicate that the knowledge embedded within \emph{Llama-2-13B-chat-GGUF} model parameters is robust, enabling effective responses to ODQA questions. Additionally, while RAG enhancement still aids accuracy, its impact is not as pronounced as in smaller models like \emph{Flan-T5-base} and \emph{Flan-T5-large}.

\emph{GPT-3.5-Turbo} showed similar performance to \emph{Llama-2-13B-chat-GGUF}, with increases in \emph{EM}, \emph{F1}, \emph{Rouge1}, and \emph{RougeL} of $3.82~(34.07 \rightarrow 37.89)$, $3.62~(39.02 \rightarrow 42.64)$, $2.02~(30.99 \rightarrow 33.01)$, and $2.09~(30.82 \rightarrow 32.91)$. With RAG knowledge enhancement, the metric improvements were slightly higher compared to those generated by \emph{Llama-2-13B-chat-GGUF}. This difference can be attributed to the fact that our ground truth answers in \emph{nq\_open} have different training time cutoffs compared to \emph{GPT-3.5-Turbo}. For instance, questions such as \texttt{‘who won last year’s NCAA women’s basketball?’} heavily depend on the model's knowledge cutoff time. Notably, \emph{GPT-3.5-Turbo} recorded the highest scores for \emph{Rouge1} and \emph{RougeL}, indicating the fluency of its responses. However, the complexity of the answers it generated might lead to them being marked as false instances in \emph{Exact Match}, causing it to slightly underperform compared to \emph{Llama-2-13B-chat-GGUF}. A better-designed instruction prompt should help increase the alignment of \emph{GPT-3.5-Turbo}’s answer generations with the format of \emph{nq\_open} answers.

\cref{tab:cases} provides additional examples of answers generated by different LLMs with knowledge augmentation.