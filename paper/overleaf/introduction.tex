\section{Introduction}

The emergence of ChatGPT and the widespread use of transformer based large-language-models (LLMs) have significantly improved many areas of NLP tasks. LLMs can converse, answer questions, reason logically, and retrieve information from its parametric stored human knowledge. Despite its impressive capabilities, LLMs such as GPTs series have notable limitations. First, it is restricted by context length, with \emph{GPT-3.5-Turbo-0613} supporting a 4096-token window~\cite{gpt3.5turbo} and \emph{GPT-4-0613} supporting an 8192-token window~\cite{gpt4turbo}. This limitation affects the model's ability to understand and remember longer text contexts. Second, LLMs are trained through pretraining and fine-tuning, which compresses knowledge into the model's parameters. Studies show that these models capture only partial knowledge~(\citet{LAMA}), and while increasing the model size can improve coverage~\citep{T5,DBLP:conf/emnlp/RobertsRS20,GPT3}, it requires more computational resources and time. Lastly, the models are limited by their knowledge cutoff, with both \emph{GPT-3.5-Turbo-0613} and \emph{GPT-4-0613}'s knowledge being up-to-date only until September 2021~\cite{gpt3.5turbo, gpt4turbo}. Updating the models to include the latest knowledge requires retraining and releasing newer versions, and the stored knowledge may not cover new or unseen data, especially in specific expert domains, leading to potential inaccuracies and hallucinations~\cite{maynez-etal-2020-faithfulness}.

In addition to encoding knowledge directly into model parameters, \emph{retrieval-augmented models}~\citep{REALM, RAG, FiD, spanlp-2022-semiparametric}, can access external sources like Wikipedia and retrieve relevant information and incorporate it into LLM’s output generation process. \cref{fig:arch} shows general workflows of \emph{retrieval-augmented generations}. This approach has significantly improved the accuracy of answering open-domain questions, where the model must respond without specific context~\citep{DBLP:conf/acl/ChenFWB17}. Furthermore, RAG can be tailored to specialized fields by creating domain-specific knowledge embeddings. These embeddings are stored in vector databases and can be retrieved based on queries, enriching the model's context for more accurate responses.

To explore the potential of retrieval-augmented approaches to enhance the Open-Domain Question Answering (ODQA) task of large language models, this study devises an external knowledge augmentation system. This system incorporates approximately 1.4 million Question-Answer pairs sourced from the \emph{PAQ} dataset~\citep{paq}, a large collection of question-answering generated from Wikipedia, as the system’s knowledge source. Utilizing the \emph{LlamaIndex} APIs, key-value embeddings for each Question-Answer pair were generated and stored within \emph{Pinecone}, a managed vector database service renowned for vector similarity search. The evaluation process uses the \emph{nq\_open}~\citep{doi:10.1162/tacl-a-00276, lee-etal-2019-latent} validation dataset, an open domain question answering benchmark that is derived from Natural Questions~\citep{lee-etal-2019-latent}. For the inference tasks pertaining to question answering, this study employs a selection of large language models: \emph{Flan-T5-base}, \emph{Flan-T5-large}, \emph{Llama-2-13B-chat}, and \emph{GPT-3.5-Turbo}, comparing the standard versions of these models with versions enhanced by \emph{PAQ} dataset knowledge. Performance is measured using \emph{Exact Match (EM)}, \emph{F1}, and \emph{Rouge scores} to evaluate the effectiveness of knowledge augmentation.
