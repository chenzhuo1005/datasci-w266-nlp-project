\section{Limitations}

Due to limited experimental time and restricted access to computational resources, this study primarily utilized Google Colab Pro with a T4 GPU. This infrastructure offers limited CPU and GPU memory and computational speed compared to more advanced systems such as the A100 or T100 GPUs. This limitation affects the choice of embedding models, as larger embedding sizes—such as OpenAI's \emph{text-embedding-3-small} (embedding size 1536) and \emph{text-embedding-3-large} (embedding size 3072)~\citep{openaiembeddings}—require significantly higher computing power and storage capacity in vector databases. However, they provide more accurate embedding searches based on similarity. This is crucial for Retrieval-Augmented Generation (RAG), as the model must retrieve the most relevant knowledge from the \emph{PAQ} dataset to accurately answer questions.

Another limitation concerns the quality of the \emph{PAQ} dataset knowledge, which may not cover all questions in the \emph{nq\_open} dataset and could potentially mislead LLMs if the retrieved answers are incorrect. An example of this issue is demonstrated in the appendix (\cref{tab:cases}) under the third question: \texttt{"How many seasons of The Bastard Executioner are there?"} The correct answer is \texttt{"one"} or \texttt{"one season"}. Initially, \emph{Llama-2-13B-chat} and \emph{GPT-3.5-Turbo} answered this question correctly using only their internally parameterized stored knowledge. However, the answers retrieved from \emph{PAQ} suggested that there were \texttt{"two"} seasons, leading the LLMs to incorrectly answer this question when using RAG. This highlights the significance of the knowledge embedding step in the retrieval augmentation process, and how poorly embedded knowledge can occasionally degrade the performance of LLMs.
